#!/bin/bash
#SBATCH -A rmittal3_gpu
#SBATCH --job-name=NeuralOpt_FSI 
#SBATCH --time=01:00:00   
#SBATCH --partition=l40s 
#SBATCH --nodes=1  
#SBATCH --cpus-per-task=16    # Request 16 CPUs per task
#SBATCH --mem=128G
#SBATCH --gres=gpu:4
#SBATCH --mail-type=all         # Email condition
#SBATCH --mail-user=skumar94@jhu.edu # Email address


# export CUDA_VISIBLE_DEVICES=0,1,2,3

export MASTER_PORT=$(expr 10000 + $(echo -n $SLURM_JOBID | tail -c 4))
export MASTER_ADDR=$(scontrol show hostnames "$SLURM_JOB_NODELIST" | head -n 1)
echo "MASTER_ADDR:MASTER_PORT="${MASTER_ADDR}:${MASTER_PORT}

export TORCH_DISTRIBUTED_DEBUG=DETAIL

# Load necessary modules
module load cuda
module load anaconda

nvidia-smi

# Activate your conda environment
conda activate /home/skumar94/.conda/envs/myenv/

# Run your Python script using torchrun
srun torchrun --nproc_per_node=4 src/training/coupledGNO_distributedTraining.py > ./logs/log.dat
# python src/training/coupledFSI_gno.py > ./logs/log.dat